{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 484 :: Data Mining :: George Mason University :: Spring 2023\n",
    "\n",
    "\n",
    "# Homework 1: KNN&PCA\n",
    "\n",
    "- **100 points [9% of your final grade]**\n",
    "- **Due Sunday, Feb 26 by 11:59pm**\n",
    "\n",
    "- *Goals of this homework:* (1) implement the KNN algorithm for classifying handwritten digit images; (2) implement the PCA algorithm to reduce the feature dimension so that we can speed up the KNN algorithm and also improve the classification performance; (3) tune the hyperparameters of the KNN and PCA algorithms to produce classification result as good as possible.\n",
    "\n",
    "- *Submission instructions:* for this homework, you need to submit to two different platforms. First, you should submit your notebook file to **Blackboard** (look for the homework 1 assignment there). Please name your submission **FirstName_Lastname_hw1.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw1.ipynb**. Your notebook should be fully executed so that we can see all outputs. Then, you need to output a txt file from this notebook (you will see later in this notebook) and submit it to the HW1 page in the http://miner2.vsnet.gmu.edu website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: KNN (40 points)\n",
    "\n",
    "In this part, you need to implement your own KNN algorithm for classifying the digits (from 0 to 9) from the handwritten digit images (28 pixel * 28 pixel). The provided train.txt is the training data you will use for building your model. Each line in the file is one sample, whose first value is the ground-truth label and the following 784 values are the pixels of the image. First of all, let's load the data by excuting the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [5 0 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = np.loadtxt(\"train.txt\", delimiter=',')\n",
    "labels = data[:, 0].astype(int)\n",
    "features = data[:, 1:]\n",
    "#print('array of labels: shape ' + str(np.shape(labels)))\n",
    "#print('array of feature matrix: shape ' + str(np.shape(features)))\n",
    "#print(f'data shape: {data.shape}')\n",
    "print(f'labels: {labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the label variable to store the ground-truth labels (from 0 to 9) of all 60,000 samples, and matrix features to store the image pixels of these samples. Next, let's excute the following code to plot the first 4 samples to see how these images look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x5000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "fig, ax = plt.subplots(1, 4, figsize=(10, 50))\n",
    "for i in range(4):  \n",
    "    ax[i].imshow(features[i].reshape((28, 28)), cmap=plt.get_cmap('gray'))\n",
    "    ax[i].set_title('Label is %d' % labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to randomly select 20% samples from the data as the **validation set**, and generate the new **training set** by removing the selected validation samples from the original dataset. Write your code in the next cell.\n",
    "\n",
    "**Note: You are NOT allowed to directly call APIs from an exiting Machine Learning library like sklearn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "SAMPLE_LENGTH = 785\n",
    "NUM_SAMPLES = 60000\n",
    "VALIDATION_SIZE = int(NUM_SAMPLES * 0.2)\n",
    "NUM_LABELS = 10\n",
    "validation_set = []\n",
    "validation_indicies = np.random.choice(NUM_SAMPLES,VALIDATION_SIZE,replace=False)\n",
    "\n",
    "for i in validation_indicies:\n",
    "    validation_set.append(data[i])\n",
    "training_set = np.delete(data,validation_indicies,axis=0)\n",
    "# TODO: set back to full set before submission\n",
    "#training_set = training_set[0:5000,:]\n",
    "#print(len(data))\n",
    "print(len(training_set))\n",
    "print(len(validation_set))\n",
    "#for i in validation_set:\n",
    "#    print(i[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to implement your KNN algorithm. In the next cell, please write your code to predict labels for samples in the validation set by the KNN model built on the training set. Here we set K = 10 and use the Euclidean distance to find neighbors.\n",
    "\n",
    "**Note: You should implement the algorithm by Python, Numpy, and other libraries you think are necessary. You are NOT allowed to directly call APIs from an exiting Machine Learning library like sklearn.**\n",
    "\n",
    "**Note: Here, you should only use the labels from the training set for the KNN model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "K = 10\n",
    "#-------------------------------------------------------------------------------\n",
    "#def get_distance(training_set,val):\n",
    "def get_distance(point1,point2):\n",
    "    #d = 0\n",
    "    #for i in range(1,len(point1)):\n",
    "    #    d = d + np.power((point1[i] - point2[i]),2)\n",
    "    #ret = np.sqrt(d)\n",
    "    p1_features = point1[1:]\n",
    "    p2_features = point2[1:]\n",
    "    #dist = np.sqrt(np.sum(np.power((p1_features-p2_features),2)))\n",
    "    dist = np.sum((p1_features - p2_features)**2)\n",
    "    #dist = np.linalg.norm(p1_features,p2_features)\n",
    "    return dist\n",
    "#-------------------------------------------------------------------------------\n",
    "# TODO: MAKE SURE I HAVE THE RIGHT SET AS PARAMETERS \n",
    "def get_nn(training_set,point,k):\n",
    "    # set empty arrays to len of validation set\n",
    "    d = []\n",
    "    for t in training_set:\n",
    "        # get distance between training point and all validation samples\n",
    "        dist = get_distance(point,t)\n",
    "        \n",
    "        #d.append((dist,point[0]))\n",
    "        di = (dist,t[0])\n",
    "        #print(type(di))\n",
    "        d.append(di)\n",
    "    #print(f'd len: {len(d)}')\n",
    "        # add current test point to array of neighbors\n",
    "    # sort and take the lowest K values\n",
    "    # TODO: make sure sort is right    \n",
    "    #neighbors = np.asarray(d)\n",
    "    #neighbors = np.sort(neighbors,axis=0)\n",
    "    neighbors = sorted(d,key=lambda x:x[0])\n",
    "    #print(neighbors)\n",
    "    neighbors = neighbors[:k]\n",
    "    #print(f'point: {point[0]}')\n",
    "    #print(f'neighbors: {neighbors[:,0]}')\n",
    "    return neighbors\n",
    "#-------------------------------------------------------------------------------\n",
    "def most_common(neighbors):\n",
    "    # take count of number neighbors with each of the NUM_LABELS labels\n",
    "    count = np.zeros(10)\n",
    "    # iterate over all neighbors\n",
    "    for n in neighbors:\n",
    "        # label is first row val as an int\n",
    "        label = n[1].astype(int)\n",
    "        # increment the corresponding index in count\n",
    "        # by 1 for each matching label\n",
    "        count[label] += 1\n",
    "    #print(f'n count: {count}')\n",
    "    return np.argmax(count)\n",
    "#-------------------------------------------------------------------------------\n",
    "def knn(training_set,validation_point,k):\n",
    "    #print(f'point is: {validation_point[0]}')    \n",
    "    n = get_nn(training_set,validation_point,k)\n",
    "    label = most_common(n)\n",
    "    #print(f'predicted: {label}')\n",
    "    return label\n",
    "#-------------------------------------------------------------------------------\n",
    "predictions = []\n",
    "# for i in validation_set:\n",
    "#     prediction = knn(training_set,i,K)\n",
    "#     #print(f'predicted label: {prediction}')\n",
    "#     predictions.append(prediction)\n",
    "# print(predictions)\n",
    "#-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, please write code to compute the Accuracy, and Micro-averaged and Macro-averaged F1 scores to evaluate the performance on the validation set and print out these three metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [31:29<00:00,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9666666666666667\n",
      "micro f1: 0.9666666666666667\n",
      "macro f1: 0.0\n",
      "global results:\n",
      "11600\n",
      "400\n",
      "400\n",
      "104400\n",
      "arr results: [[ 1127    33    10 10863     0]\n",
      " [ 1366    74    12 10622     0]\n",
      " [ 1173    19    63 10764     0]\n",
      " [ 1192    52    50 10758     0]\n",
      " [ 1121    27    38 10841     0]\n",
      " [ 1006    48    38 10956     0]\n",
      " [ 1193    27    23 10784     0]\n",
      " [ 1227    57    25 10748     0]\n",
      " [ 1048    21    76 10876     0]\n",
      " [ 1147    42    65 10788     0]]\n",
      "check 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# total for all samples\n",
    "true_pos = 0  #label and image match\n",
    "false_pos = 0 # label matches\n",
    "false_neg = 0 # image matches\n",
    "true_neg = 0 # no matches\n",
    "\n",
    "# array to keep track of each result for each label\n",
    "# each element is [TP,FP,FN,TN, F1]\n",
    "label_results = np.array([\n",
    "    [0,0,0,0,0], # 0\n",
    "    [0,0,0,0,0], # 1\n",
    "    [0,0,0,0,0], # 2\n",
    "    [0,0,0,0,0], # 3\n",
    "    [0,0,0,0,0], # 4\n",
    "    [0,0,0,0,0], # 5\n",
    "    [0,0,0,0,0], # 6\n",
    "    [0,0,0,0,0], # 7\n",
    "    [0,0,0,0,0], # 8\n",
    "    [0,0,0,0,0], # 9\n",
    "])\n",
    "check = 0\n",
    "for val in tqdm(validation_set):\n",
    "    knn_label = knn(training_set,val,K).astype(int) # model's prediction for label\n",
    "    actual_label = val[0].astype(int)\n",
    "    #print(knn_label == actual_label)\n",
    "    # if knn = actual - TP fir label_results[actual]\n",
    "    if knn_label == actual_label:\n",
    "        #print(f'!')\n",
    "        label_results[actual_label][0] += 1 # inc TP for label\n",
    "        true_pos += 1\n",
    "        # inc TN for other labels\n",
    "        for i in range(len(label_results)):\n",
    "            if i != actual_label:\n",
    "                label_results[i][3] += 1\n",
    "                true_neg += 1\n",
    "    else:\n",
    "        #print(f'-')\n",
    "        check += 1\n",
    "        for i in range(len(label_results)):\n",
    "            # for knn label: inc FP\n",
    "            if i == knn_label:\n",
    "                label_results[i][1] += 1\n",
    "                false_pos += 1\n",
    "            # for act label: inc FN\n",
    "            if i == actual_label:\n",
    "                label_results[i][2] += 1\n",
    "                false_neg += 1\n",
    "            # for others inc TN\n",
    "            else:\n",
    "                label_results[i][3] += 1\n",
    "#-------------------------------------------------------------------------------\n",
    "def precision(tp,fp):\n",
    "    return ((tp)/(tp+fp))\n",
    "#-------------------------------------------------------------------------------\n",
    "def recall(tp,fn):\n",
    "    return ((tp)/(tp+fn))\n",
    "#-------------------------------------------------------------------------------\n",
    "def f1_score(tp,fp,fn):\n",
    "    return (2 * ((precision(tp,fp) * recall(tp,fn))/(precision(tp,fp) + recall(tp,fn))))\n",
    "#-------------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "#----calculate accuracy----\n",
    "accuracy = true_pos / VALIDATION_SIZE\n",
    "\n",
    "#----calculate micro----\n",
    "micro_f1 = f1_score(true_pos,false_pos,false_neg)\n",
    "#----calculate macro----\n",
    "# calculate f1 for each label\n",
    "for label in label_results:\n",
    "    print(label)\n",
    "    label[4] = f1_score(label[0],label[1],label[2])\n",
    "sums = np.sum(label_results,axis=0)\n",
    "macro_f1 = sums[-1] / NUM_LABELS\n",
    "\n",
    "print(f'accuracy: {accuracy}')\n",
    "print(f'micro f1: {micro_f1}')\n",
    "print(f'macro f1: {macro_f1}')\n",
    "\n",
    "print(f'global results:\\n{true_pos}\\n{false_pos}\\n{false_neg}\\n{true_neg}')\n",
    "print(f'arr results: {label_results}')\n",
    "print(f'check {check}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "micro_precision = ((tp1+tp2)/(tp1+fp1+tp2+fp2))\n",
    "micro_recall = ((tp1+tp2)/(tp1+fn1+tp2+fn2))\n",
    "micro_f1 = 2 * ((micro_precis * micro_recall)/(micro_precis + micro_recall))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: PCA (30 points)\n",
    "\n",
    "In this part, you will implement the PCA algorithm to reduce the input dimension for the handwritten digit recognition task. In the next cell, please write your code to compute the transformation matrix in the PCA method for the training set we got from the previous part. Here, we only keep the **top 50 dimensions**.\n",
    "\n",
    "**Hint: You can use the function from the Numpy library to compute SVD:**\n",
    "\n",
    "*u, s, v = np.linalg.svd(a, full_matrices=False)*\n",
    "\n",
    "\n",
    "**Note: You should only use the training set to compute PCA without using validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels: (48000,)\n",
      "xTrain shape: (48000, 784)\n",
      "u shape: (48000, 784)\n",
      "g shape: (784, 50)\n",
      "gT shape: (50, 784)\n",
      "zTrain: (50, 48000)\n",
      "ZVal: (50, 12000)\n",
      "zTrainT shape: (48000, 50)\n",
      "FTV: (48000, 51)\n",
      "FVT shape: (12000, 51)\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "REDUCED_DIMENSION = 50\n",
    "ORIGINAL_DIMENSION = 784\n",
    "#-------------------------------------------------------------------------------\n",
    "'''\n",
    "convert validation set to np array\n",
    "\n",
    "make copies of sets\n",
    "\n",
    "remove labels\n",
    "\n",
    "center training (set - means,axis=0)\n",
    "\n",
    "get U from SVD(xTrain)\n",
    "\n",
    "slice U into G\n",
    "\n",
    "center validation\n",
    "\n",
    "pca g.T with both sets\n",
    "'''\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# make copies to modify without reslicing same set over and over when cell is run\n",
    "training_copy = training_set\n",
    "validation_copy = np.asarray(validation_set) # turn arr into nparr\n",
    "# store labels in case we need them later\n",
    "validation_labels = validation_copy[:,0]\n",
    "training_labels = training_copy[:,0]\n",
    "print(f'training labels: {training_labels.shape}')\n",
    "# remove labels\n",
    "training_copy = training_copy[:,1:]\n",
    "validation_copy = validation_copy[:,1:]\n",
    "# center training set\n",
    "training_means = np.mean(training_copy,axis=0)\n",
    "xTrain = training_copy - training_means\n",
    "print(f'xTrain shape: {xTrain.shape}')\n",
    "# get U from svd(xTrain)\n",
    "u,s,v = np.linalg.svd(xTrain,full_matrices=False)\n",
    "print(f'u shape: {u.shape}')\n",
    "# slice u into g\n",
    "u = u.T\n",
    "g = u[:,:50]\n",
    "print(f'g shape: {g.shape}')\n",
    "gT = g.T\n",
    "print(f'gT shape: {gT.shape}')\n",
    "# center validation\n",
    "validation_means = np.mean(validation_copy,axis=0)\n",
    "xValidation = validation_copy - validation_means\n",
    "\n",
    "zTrain = gT @ xTrain.T\n",
    "zValidation = gT @ xValidation.T\n",
    "\n",
    "print(f'zTrain: {zTrain.shape}')\n",
    "print(f'ZVal: {zValidation.shape}')\n",
    "\n",
    "zTrainT = zTrain.T\n",
    "zValidationT = zValidation.T\n",
    "print(f'zTrainT shape: {zTrainT.shape}')\n",
    "\n",
    "#newTraining = zip(training_labels,zTrainT)\n",
    "#newValidation = zip(validation_labels,zValidationT)\n",
    "fullZTraining = np.zeros([(NUM_SAMPLES-VALIDATION_SIZE),(REDUCED_DIMENSION+1)])\n",
    "for i in range(len(fullZTraining)):\n",
    "    #print(f'ztrain row: {zTrainT[i]}')\n",
    "    fullZTraining[i][0] = training_labels[i]\n",
    "    for j in range(len(zTrainT[i])):\n",
    "        fullZTraining[i][j+1] = zTrainT[i,j]\n",
    "print(f'FTV: {fullZTraining.shape}')\n",
    "#print(fullZTraining[0])\n",
    "\n",
    "fullZValidation = np.zeros([VALIDATION_SIZE,(REDUCED_DIMENSION+1)])\n",
    "for i in range(len(fullZValidation)):\n",
    "    fullZValidation[i][0] = validation_labels[i]\n",
    "    for j in range(len(zValidationT[i])):\n",
    "        fullZValidation[i][j+1] = zValidationT[i,j]\n",
    "print(f'FVT shape: {fullZValidation.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to apply the computed transformation matrix to reduce the dimension for the training set and the validation set. Then, build a new KNN model on the dimension-reduced traning data and predict the labels for the dimension-reduced validation set. Report the Accuracy, and Micro-averaged and Macro-averaged F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [31:52<00:00,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9666666666666667\n",
      "micro f1: 0.9666666666666667\n",
      "macro f1: 0.0\n",
      "global results:\n",
      "11600\n",
      "400\n",
      "400\n",
      "104400\n",
      "arr results: [[ 1127    33    10 10863     0]\n",
      " [ 1366    74    12 10622     0]\n",
      " [ 1173    19    63 10764     0]\n",
      " [ 1192    52    50 10758     0]\n",
      " [ 1121    27    38 10841     0]\n",
      " [ 1006    48    38 10956     0]\n",
      " [ 1193    27    23 10784     0]\n",
      " [ 1227    57    25 10748     0]\n",
      " [ 1048    21    76 10876     0]\n",
      " [ 1147    42    65 10788     0]]\n",
      "check 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# total for all samples\n",
    "true_pos = 0  #label and image match\n",
    "false_pos = 0 # label matches\n",
    "false_neg = 0 # image matches\n",
    "true_neg = 0 # no matches\n",
    "\n",
    "# array to keep track of each result for each label\n",
    "# each element is [TP,FP,FN,TN, F1]\n",
    "label_results = np.array([\n",
    "    [0,0,0,0,0], # 0\n",
    "    [0,0,0,0,0], # 1\n",
    "    [0,0,0,0,0], # 2\n",
    "    [0,0,0,0,0], # 3\n",
    "    [0,0,0,0,0], # 4\n",
    "    [0,0,0,0,0], # 5\n",
    "    [0,0,0,0,0], # 6\n",
    "    [0,0,0,0,0], # 7\n",
    "    [0,0,0,0,0], # 8\n",
    "    [0,0,0,0,0], # 9\n",
    "])\n",
    "check = 0\n",
    "for val in tqdm(validation_set):\n",
    "    knn_label = knn(training_set,val,K).astype(int) # model's prediction for label\n",
    "    actual_label = val[0].astype(int)\n",
    "    #print(knn_label == actual_label)\n",
    "    # if knn = actual - TP fir label_results[actual]\n",
    "    if knn_label == actual_label:\n",
    "        #print(f'!')\n",
    "        label_results[actual_label][0] += 1 # inc TP for label\n",
    "        true_pos += 1\n",
    "        # inc TN for other labels\n",
    "        for i in range(len(label_results)):\n",
    "            if i != actual_label:\n",
    "                label_results[i][3] += 1\n",
    "                true_neg += 1\n",
    "    else:\n",
    "        #print(f'-')\n",
    "        check += 1\n",
    "        for i in range(len(label_results)):\n",
    "            # for knn label: inc FP\n",
    "            if i == knn_label:\n",
    "                label_results[i][1] += 1\n",
    "                false_pos += 1\n",
    "            # for act label: inc FN\n",
    "            if i == actual_label:\n",
    "                label_results[i][2] += 1\n",
    "                false_neg += 1\n",
    "            # for others inc TN\n",
    "            else:\n",
    "                label_results[i][3] += 1\n",
    "#-------------------------------------------------------------------------------\n",
    "def precision(tp,fp):\n",
    "    return ((tp)/(tp+fp))\n",
    "#-------------------------------------------------------------------------------\n",
    "def recall(tp,fn):\n",
    "    return ((tp)/(tp+fn))\n",
    "#-------------------------------------------------------------------------------\n",
    "def f1_score(tp,fp,fn):\n",
    "    return (2 * ((precision(tp,fp) * recall(tp,fn))/(precision(tp,fp) + recall(tp,fn))))\n",
    "#-------------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "#----calculate accuracy----\n",
    "accuracy = true_pos / VALIDATION_SIZE\n",
    "\n",
    "#----calculate micro----\n",
    "micro_f1 = f1_score(true_pos,false_pos,false_neg)\n",
    "#----calculate macro----\n",
    "# calculate f1 for each label\n",
    "for label in label_results:\n",
    "    print(label)\n",
    "    label[4] = f1_score(label[0],label[1],label[2])\n",
    "sums = np.sum(label_results,axis=0)\n",
    "macro_f1 = sums[-1] / NUM_LABELS\n",
    "\n",
    "print(f'accuracy: {accuracy}')\n",
    "print(f'micro f1: {micro_f1}')\n",
    "print(f'macro f1: {macro_f1}')\n",
    "\n",
    "print(f'global results:\\n{true_pos}\\n{false_pos}\\n{false_neg}\\n{true_neg}')\n",
    "print(f'arr results: {label_results}')\n",
    "print(f'check {check}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Tune Hyperparameter [Need to submit to Miner2] (30 points)\n",
    "\n",
    "In this part, you need to do your best to tune the hyperparameter in KNN and PCA to build the best model and submit the predictions for the testing data to Miner2 system. First of all, let's load the testing data by excuting the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of testing feature matrix: shape (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "test_features = np.loadtxt(\"test.txt\", delimiter=',')\n",
    "print('array of testing feature matrix: shape ' + str(np.shape(test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should tune three hyperparameters:\n",
    "\n",
    "- the number of nearest neighbors in KNN \n",
    "- the distance measurement (choose from Euclidean distance, L1 norm distance, and cosine distance)\n",
    "- the number of dimensions kept in PCA \n",
    "\n",
    "Rules:\n",
    "\n",
    "- Write your predictions for samples in the testing set into a file, in which each line has one integer indicating the prediction from your best model for the corresponding sample in the test.txt file. Please see the format.txt file in Miner2 as one submission example. Name the submission file hw1_Miner2.txt and submit it to Miner2 HW1 page.\n",
    "- The public leaderboard shows results for 50% of randomly chosen test instances only. This is a standard practice in data mining challenge to avoid gaming of the system. The private leaderboard will be released after the deadline evaluates all the entries in the test set.\n",
    "- You are allowed 5 submissions in a 24 hour cycle. \n",
    "- The final score and ranking will always be based on the last submission.\n",
    "- Grading will only be based on the model performance (based on Accuracy metric) instead of ranking. You'll get full credit as long as your socre is a reasonable number.\n",
    "\n",
    "\n",
    "**Hint: You can tune these hyperparameters by one randomly generated validation set (like what you have done in previous parts), or you can also use the cross-validation method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels: (48000,)\n",
      "xTrain shape: (48000, 784)\n",
      "u shape: (48000, 784)\n",
      "g shape: (784, 50)\n",
      "gT shape: (50, 784)\n",
      "zTrain: (50, 48000)\n",
      "ZVal: (50, 10000)\n",
      "zTrainT shape: (48000, 50)\n",
      "FTV: (48000, 51)\n",
      "FVT shape: (10000, 51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 1360/10000 [03:26<22:11,  6.49it/s]"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "from scipy.spatial import distance\n",
    "K_NEIGHBORS = 10\n",
    "SAMPLE_LENGTH = 784\n",
    "VALIDATION_SIZE = 10000\n",
    "REDUCED_DIMENSION = 50\n",
    "\n",
    "validation_set = test_features\n",
    "\n",
    "################################################################################\n",
    "#                                   KNN\n",
    "################################################################################\n",
    "\n",
    "# Write your code here\n",
    "#K = 10\n",
    "#-------------------------------------------------------------------------------\n",
    "#def get_distance(training_set,val):\n",
    "def get_distance(point1,point2):\n",
    "    #d = 0\n",
    "    #for i in range(1,len(point1)):\n",
    "    #    d = d + np.power((point1[i] - point2[i]),2)\n",
    "    #ret = np.sqrt(d)\n",
    "    p1_features = point1\n",
    "    p2_features = point2[1:]\n",
    "    #dist = np.sqrt(np.sum(np.power((p1_features-p2_features),2)))\n",
    "    dist = np.sum((p1_features - p2_features)**2)\n",
    "    \n",
    "    \n",
    "    # TODO: USE THIS\n",
    "    #dist = distance.cosine(point1,point2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #dist = np.linalg.norm(p1_features,p2_features)\n",
    "    return dist\n",
    "#-------------------------------------------------------------------------------\n",
    "# TODO: MAKE SURE I HAVE THE RIGHT SET AS PARAMETERS \n",
    "def get_nn(training_set,point,k):\n",
    "    # set empty arrays to len of validation set\n",
    "    d = []\n",
    "    for t in training_set:\n",
    "        # get distance between training point and all validation samples\n",
    "        dist = get_distance(point,t)\n",
    "        \n",
    "        #d.append((dist,point[0]))\n",
    "        di = (dist,t[0])\n",
    "        #print(type(di))\n",
    "        d.append(di)\n",
    "    #print(f'd len: {len(d)}')\n",
    "        # add current test point to array of neighbors\n",
    "    # sort and take the lowest K values\n",
    "    # TODO: make sure sort is right    \n",
    "    #neighbors = np.asarray(d)\n",
    "    #neighbors = np.sort(neighbors,axis=0)\n",
    "    neighbors = sorted(d,key=lambda x:x[0])\n",
    "    #print(neighbors)\n",
    "    neighbors = neighbors[:K_NEIGHBORS]\n",
    "    #print(f'point: {point[0]}')\n",
    "    #print(f'neighbors: {neighbors[:,0]}')\n",
    "    return neighbors\n",
    "#-------------------------------------------------------------------------------\n",
    "def most_common(neighbors):\n",
    "    # take count of number neighbors with each of the NUM_LABELS labels\n",
    "    count = np.zeros(10)\n",
    "    # iterate over all neighbors\n",
    "    for n in neighbors:\n",
    "        # label is first row val as an int\n",
    "        label = n[1].astype(int)\n",
    "        # increment the corresponding index in count\n",
    "        # by 1 for each matching label\n",
    "        count[label] += 1\n",
    "    #print(f'n count: {count}')\n",
    "    return np.argmax(count)\n",
    "#-------------------------------------------------------------------------------\n",
    "def knn(training_set,validation_point,k):\n",
    "    #print(f'point is: {validation_point[0]}')    \n",
    "    n = get_nn(training_set,validation_point,k)\n",
    "    label = most_common(n)\n",
    "    #print(f'predicted: {label}')\n",
    "    return label\n",
    "#-------------------------------------------------------------------------------\n",
    "predictions = []\n",
    "# for i in validation_set:\n",
    "#     prediction = knn(training_set,i,K)\n",
    "#     #print(f'predicted label: {prediction}')\n",
    "#     predictions.append(prediction)\n",
    "# print(predictions)\n",
    "#-------------------------------------------------------------------------------\n",
    "################################################################################\n",
    "#                                   PCA\n",
    "################################################################################\n",
    "# make copies to modify without reslicing same set over and over when cell is run\n",
    "training_copy = training_set\n",
    "validation_copy = np.asarray(validation_set) # turn arr into nparr\n",
    "# store labels in case we need them later\n",
    "validation_labels = validation_copy[:,0]\n",
    "training_labels = training_copy[:,0]\n",
    "print(f'training labels: {training_labels.shape}')\n",
    "# remove labels\n",
    "training_copy = training_copy[:,1:]\n",
    "#validation_copy = validation_copy[:,1:]\n",
    "# center training set\n",
    "training_means = np.mean(training_copy,axis=0)\n",
    "xTrain = training_copy - training_means\n",
    "print(f'xTrain shape: {xTrain.shape}')\n",
    "# get U from svd(xTrain)\n",
    "u,s,v = np.linalg.svd(xTrain,full_matrices=False)\n",
    "print(f'u shape: {u.shape}')\n",
    "# slice u into g\n",
    "u = u.T\n",
    "g = u[:,:50]\n",
    "print(f'g shape: {g.shape}')\n",
    "gT = g.T\n",
    "print(f'gT shape: {gT.shape}')\n",
    "# center validation\n",
    "validation_means = np.mean(validation_copy,axis=0)\n",
    "xValidation = validation_copy - validation_means\n",
    "\n",
    "zTrain = gT @ xTrain.T\n",
    "zValidation = gT @ xValidation.T\n",
    "\n",
    "print(f'zTrain: {zTrain.shape}')\n",
    "print(f'ZVal: {zValidation.shape}')\n",
    "\n",
    "zTrainT = zTrain.T\n",
    "zValidationT = zValidation.T\n",
    "print(f'zTrainT shape: {zTrainT.shape}')\n",
    "\n",
    "#newTraining = zip(training_labels,zTrainT)\n",
    "#newValidation = zip(validation_labels,zValidationT)\n",
    "fullZTraining = np.zeros([(48000),(REDUCED_DIMENSION+1)])\n",
    "for i in range(len(fullZTraining)):\n",
    "    #print(f'ztrain row: {zTrainT[i]}')\n",
    "    fullZTraining[i][0] = training_labels[i]\n",
    "    for j in range(len(zTrainT[i])):\n",
    "        fullZTraining[i][j+1] = zTrainT[i,j]\n",
    "print(f'FTV: {fullZTraining.shape}')\n",
    "#print(fullZTraining[0])\n",
    "\n",
    "fullZValidation = np.zeros([VALIDATION_SIZE,(REDUCED_DIMENSION+1)])\n",
    "for i in range(len(fullZValidation)):\n",
    "    fullZValidation[i][0] = validation_labels[i]\n",
    "    for j in range(len(zValidationT[i])):\n",
    "        fullZValidation[i][j+1] = zValidationT[i,j]\n",
    "print(f'FVT shape: {fullZValidation.shape}')\n",
    "################################################################################\n",
    "#                                   RUN STUFF\n",
    "################################################################################\n",
    "# total for all samples\n",
    "true_pos = 0  #label and image match\n",
    "false_pos = 0 # label matches\n",
    "false_neg = 0 # image matches\n",
    "true_neg = 0 # no matches\n",
    "\n",
    "# array to keep track of each result for each label\n",
    "# each element is [TP,FP,FN,TN, F1]\n",
    "label_results = np.array([\n",
    "    [0,0,0,0,0], # 0\n",
    "    [0,0,0,0,0], # 1\n",
    "    [0,0,0,0,0], # 2\n",
    "    [0,0,0,0,0], # 3\n",
    "    [0,0,0,0,0], # 4\n",
    "    [0,0,0,0,0], # 5\n",
    "    [0,0,0,0,0], # 6\n",
    "    [0,0,0,0,0], # 7\n",
    "    [0,0,0,0,0], # 8\n",
    "    [0,0,0,0,0], # 9\n",
    "])\n",
    "check = 0\n",
    "# array to write labels to\n",
    "p3_labels = []\n",
    "for val in tqdm(validation_set):\n",
    "    knn_label = knn(training_set,val,K_NEIGHBORS).astype(int) # model's prediction for label\n",
    "    p3_labels.append(knn_label)\n",
    "    actual_label = val[0].astype(int)\n",
    "    #print(knn_label == actual_label)\n",
    "    # if knn = actual - TP fir label_results[actual]\n",
    "    if knn_label == actual_label:\n",
    "        #print(f'!')\n",
    "        label_results[actual_label][0] += 1 # inc TP for label\n",
    "        true_pos += 1\n",
    "        # inc TN for other labels\n",
    "        for i in range(len(label_results)):\n",
    "            if i != actual_label:\n",
    "                label_results[i][3] += 1\n",
    "                true_neg += 1\n",
    "    else:\n",
    "        #print(f'-')\n",
    "        check += 1\n",
    "        for i in range(len(label_results)):\n",
    "            # for knn label: inc FP\n",
    "            if i == knn_label:\n",
    "                label_results[i][1] += 1\n",
    "                false_pos += 1\n",
    "            # for act label: inc FN\n",
    "            if i == actual_label:\n",
    "                label_results[i][2] += 1\n",
    "                false_neg += 1\n",
    "            # for others inc TN\n",
    "            else:\n",
    "                label_results[i][3] += 1\n",
    "#-------------------------------------------------------------------------------\n",
    "def precision(tp,fp):\n",
    "    return ((tp)/(tp+fp))\n",
    "#-------------------------------------------------------------------------------\n",
    "def recall(tp,fn):\n",
    "    return ((tp)/(tp+fn))\n",
    "#-------------------------------------------------------------------------------\n",
    "def f1_score(tp,fp,fn):\n",
    "    return (2 * ((precision(tp,fp) * recall(tp,fn))/(precision(tp,fp) + recall(tp,fn))))\n",
    "#-------------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "#----calculate accuracy----\n",
    "accuracy = true_pos / VALIDATION_SIZE\n",
    "\n",
    "#----calculate micro----\n",
    "micro_f1 = f1_score(true_pos,false_pos,false_neg)\n",
    "#----calculate macro----\n",
    "# calculate f1 for each label\n",
    "# for label in label_results:\n",
    "#     print(label)\n",
    "#     label[4] = f1_score(label[0],label[1],label[2])\n",
    "sums = np.sum(label_results,axis=0)\n",
    "macro_f1 = sums[-1] / NUM_LABELS\n",
    "\n",
    "print(f'accuracy: {accuracy}')\n",
    "print(f'micro f1: {micro_f1}')\n",
    "print(f'macro f1: {macro_f1}')\n",
    "\n",
    "print(f'global results:\\n{true_pos}\\n{false_pos}\\n{false_neg}\\n{true_neg}')\n",
    "print(f'arr results: {label_results}')\n",
    "print(f'check {check}')\n",
    "\n",
    "with open('hw1_Miner2.txt','w') as f:\n",
    "    for label in p3_labels:\n",
    "        f.write(f'{label}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is your final hyperparameter setting? How do you tune them? What choices have you tried?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My final settings were the default ones, as that was all I had time to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: your username in Miner2 and the score&ranking of your submission in Miner2 (at the time of answering this question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HD_Pickles - did not get file posted in time because I ran on the original dataset instead of the new 'test_features' set\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
